experiment_name: "gpt2_finetune_for_instructions"
output_dir: ./outputs/${experiment_name}

hydra:
  run:
    dir: ${output_dir}

version: 1.1.0
defaults:
  - _self_

logging:
  file: "run_hydra.log"

device:
  use_cuda: auto
  seed: 123

data:
  url: "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json"
  local_file: ${output_dir}/instruction-data.json
  train_ratio: 0.80
  test_ratio: 0.10
  num_workers: 4

model:
  model_name: "gpt2-small (124M)" # "gpt2-small (124M)", "gpt2-medium (355M)", "gpt2-large (774M)", "gpt2-xl (1558M)"
  vocab_size: 50257
  context_length: 256
  emb_dim: 768
  n_heads: 12
  n_layers: 12
  drop_rate: 0.1
  qkv_bias: False
  load_pretrained_hf_model: true
  fine_tune: true
  num_classes: 2

tiktoken:
  model_name: "gpt2"

training:
  learning_rate: 5e-4
  weight_decay: 0.1
  batch_size: 2
  num_epochs: 2
  eval_freq: 5
  eval_iter: 1
  start_context: "Every effort moves you"

test_mode: true